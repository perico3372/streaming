{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92fb0d2-3bfe-4c7e-9041-baf3713bf4f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.local/share/virtualenvs/workSpace-Bwo4fINe/lib/python3.11/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# importar módulos\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import col, sum, lower, lit, when, avg, count, isnan, to_date, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac30b79e-ea87-4b45-b0ab-6d7c991d84d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# definir variables de entorno\n",
    "\n",
    "sparkHome = \"/home/pablo/hadoop/spark-3.5.0-bin-hadoop3\"\n",
    "os.environ['SPARK_HOME'] = sparkHome\n",
    "\n",
    "javaHome = \"/opt/openjdk-bin-17.0.7_p7\"\n",
    "os.environ['JAVA_HOME'] = javaHome\n",
    "\n",
    "path = \"/home/pablo/hadoop/spark-3.5.0-bin-hadoop3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/bin:/usr/lib/llvm/16/bin:/usr/lib/llvm/14/bin\"\n",
    "\n",
    "os.environ['PATH'] = f'{sparkHome}/bin:{os.environ[\"PATH\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c80edbb7-c652-4d23-a10a-aafbe639c43a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 14:49:05 WARN Utils: Your hostname, gentoo resolves to a loopback address: 127.0.0.1; using 192.168.0.199 instead (on interface br0)\n",
      "23/09/25 14:49:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/25 14:49:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# crear sesión\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0419600c-65bc-416f-9060-2a801ca48cec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.199:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f51544dc290>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2acc56d9-ca23-4934-81ce-c5d4f3241fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url \n",
    "\n",
    "url1 = \"https://gitlab.com/perico3372/data/-/raw/main/streaming/ratings_01.csv\"\n",
    "url2 = \"https://gitlab.com/perico3372/data/-/raw/main/streaming/ratings_02.csv\"\n",
    "url3 = \"https://gitlab.com/perico3372/data/-/raw/main/streaming/ratings_03.csv\"\n",
    "url4 = \"https://gitlab.com/perico3372/data/-/raw/main/streaming/ratings_04.csv\"\n",
    "url5 = \"https://gitlab.com/perico3372/data/-/raw/main/streaming/ratings_05.csv\"\n",
    "url6 = \"https://gitlab.com/perico3372/data/-/raw/main/streaming/ratings_06.csv\"\n",
    "url7 = \"https://gitlab.com/perico3372/data/-/raw/main/streaming/ratings_07.csv\"\n",
    "url8 = \"https://gitlab.com/perico3372/data/-/raw/main/streaming/ratings_08.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df8a2893-33e9-4bbf-ac49-700ead28592f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pablo/.local/share/virtualenvs/workSpace-Bwo4fINe/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pablo/.local/share/virtualenvs/workSpace-Bwo4fINe/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39maddFile(url3)\n\u001b[1;32m      6\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39maddFile(url4)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39maddFile(url6)\n\u001b[1;32m      9\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39maddFile(url7)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workSpace-Bwo4fINe/lib/python3.11/site-packages/pyspark/context.py:1894\u001b[0m, in \u001b[0;36mSparkContext.addFile\u001b[0;34m(self, path, recursive)\u001b[0m\n\u001b[1;32m   1819\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maddFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, recursive: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1820\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1821\u001b[0m \u001b[38;5;124;03m    Add a file to be downloaded with this Spark job on every node.\u001b[39;00m\n\u001b[1;32m   1822\u001b[0m \u001b[38;5;124;03m    The `path` passed can be either a local file, a file in HDFS\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;124;03m    [100, 200, 300, 400]\u001b[39;00m\n\u001b[1;32m   1893\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1894\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workSpace-Bwo4fINe/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workSpace-Bwo4fINe/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workSpace-Bwo4fINe/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# agregar archivos al contexto\n",
    "\n",
    "spark.sparkContext.addFile(url1)\n",
    "spark.sparkContext.addFile(url2)\n",
    "spark.sparkContext.addFile(url3)\n",
    "spark.sparkContext.addFile(url4)\n",
    "spark.sparkContext.addFile(url5)\n",
    "spark.sparkContext.addFile(url6)\n",
    "spark.sparkContext.addFile(url7)\n",
    "spark.sparkContext.addFile(url8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2138a1f3-fffb-4f9c-9395-84826eacb882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataframes ratings\n",
    "\n",
    "dataFrame1 = spark.read.csv(SparkFiles.get(\"ratings_01.csv\"), header=True, inferSchema=True)\n",
    "dataFrame2 = spark.read.csv(SparkFiles.get(\"ratings_02.csv\"), header=True, inferSchema=True)\n",
    "dataFrame3 = spark.read.csv(SparkFiles.get(\"ratings_03.csv\"), header=True, inferSchema=True)\n",
    "dataFrame4 = spark.read.csv(SparkFiles.get(\"ratings_04.csv\"), header=True, inferSchema=True)\n",
    "dataFrame5 = spark.read.csv(SparkFiles.get(\"ratings_05.csv\"), header=True, inferSchema=True)\n",
    "dataFrame6 = spark.read.csv(SparkFiles.get(\"ratings_06.csv\"), header=True, inferSchema=True)\n",
    "dataFrame7 = spark.read.csv(SparkFiles.get(\"ratings_07.csv\"), header=True, inferSchema=True)\n",
    "dataFrame8 = spark.read.csv(SparkFiles.get(\"ratings_08.csv\"), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf30c6b-08e6-4175-bf4a-79537cb8d9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unir dataframe\n",
    "\n",
    "ratings = dataFrame1.union(dataFrame2).union(dataFrame3).union(dataFrame4).union(dataFrame5).union(dataFrame6).union(dataFrame7).union(dataFrame8)\n",
    "ratings = ratings.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a424f79-7a3c-48ba-9a8a-301e85712e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8cef7c-d50c-4d2f-b5f1-365d1663389b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "ratings.sample(False, 3/ratings.count(), seed=300).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b00fc5-a4ac-4929-af9f-c924e42ac0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee396df-b580-483d-b8d2-e52462c48d31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transformar a minusculas nombre columnas ratings\n",
    "\n",
    "ratings = ratings.withColumnRenamed(\"userId\", \"userid\").withColumnRenamed(\"movieId\", \"movieid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465615ed-f8da-4b6c-ac5e-4aa26617e45b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings_scored = ratings.groupBy(\"movieId\").agg(avg(\"rating\").alias(\"scored\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4ba1c-18f2-483e-9e36-7cf4bdc8d233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transformar a minusculas nombre columnas ratings\n",
    "\n",
    "ratings_scored = ratings_scored.withColumnRenamed(\"movieId\", \"movieid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f592ae0-d413-4646-ae61-bbd4977e272c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings_scored.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70cf44-4761-444b-9374-5f1dfa2b7574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "ratings_scored.sample(False, 3/ratings_scored.count(), seed=0).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a1c584-25c3-42cb-ab90-18693437a58c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transformar el campo rating de tipo de dato string a tippo de dato a float 32 bits\n",
    "\n",
    "ratings = ratings.withColumn('rating', col('rating').cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0badeb-78e7-4af4-857e-785a532b9bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in ratings.columns:\n",
    "    print(col, \"El tipo de dato del campo:\", ratings.schema[col].dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e352a0f-c59b-46c5-80fa-cf3e12d9b219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# renombrar el nombre de la columna rating en el ratings con el nombre de scored\n",
    "\n",
    "ratings = ratings.withColumnRenamed('rating', 'scored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b050f9-057b-4271-a02d-36c273b5bc95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#listar valores nulos por columna\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "ratings.select(*(sum(col(c).isNull().cast('int')).alias(c) for c in ratings.columns)).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76556b-eb64-4714-8228-d78553ba5a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# listar dimensiones de dataframe ratings\n",
    "\n",
    "num_filas = ratings.count()\n",
    "num_columnas = len(ratings.columns)\n",
    "print(\"Las dimensiones del DataFrame ratings son: ({}, {})\".format(num_filas, num_columnas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f502777-ab1c-416d-8901-ab23b6cdb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url \n",
    "\n",
    "urlAmazon = \"https://gitlab.com/perico3372/data/-/raw/main/streaming/amazon_prime_titles.csv\"\n",
    "urlDisneyPlus = \"https://gitlab.com/perico3372/data/-/raw/main/streaming/disney_plus_titles.csv\"\n",
    "urlHulu = \"https://gitlab.com/perico3372/data/-/raw/main/streaming/hulu_titles.csv\"\n",
    "urlNetflix = \"https://gitlab.com/perico3372/data/-/raw/main/streaming/netflix_titles.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a53e0-d417-4e95-8e14-1bcccdbd45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agregar archivos al contexto\n",
    "\n",
    "spark.sparkContext.addFile(urlAmazon)\n",
    "spark.sparkContext.addFile(urlDisneyPlus)\n",
    "spark.sparkContext.addFile(urlHulu)\n",
    "spark.sparkContext.addFile(urlNetflix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6482ab8d-921c-4118-b9cd-05ea6dd996e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes\n",
    "\n",
    "amazon = spark.read.csv(SparkFiles.get(\"amazon_prime_titles.csv\"), header=True, inferSchema=True)\n",
    "disneyPlus = spark.read.csv(SparkFiles.get(\"disney_plus_titles.csv\"), header=True, inferSchema=True)\n",
    "hulu = spark.read.csv(SparkFiles.get(\"hulu_titles.csv\"), header=True, inferSchema=True)\n",
    "netflix = spark.read.csv(SparkFiles.get(\"netflix_titles.csv\"), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8cfba6-cf65-431b-b774-c0b1756799ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# agregar columna plataform a dataframe \n",
    "\n",
    "amazon = titles1.withColumn(\"platform\", lit(\"amazon\"))\n",
    "disneyPlus = titles2.withColumn(\"platform\", lit(\"disney\"))\n",
    "hulu = titles3.withColumn(\"platform\", lit(\"hulu\"))\n",
    "netflix = Netflix.withColumn(\"platform\", lit(\"netflix\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778eef27-544f-4941-b993-a537a1213791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generar campo id: Cada id se compondrá de la primera letra del nombre de la plataforma, seguido del show_id\n",
    "\n",
    "from pyspark.sql.functions import concat, lit, col\n",
    "\n",
    "amazon = titles1.withColumn(\"show_id\", concat(lit(\"a\"), col(\"show_id\")))\n",
    "disneyPlus = titles2.withColumn(\"show_id\", concat(lit(\"d\"), col(\"show_id\")))\n",
    "hulu = titles3.withColumn(\"show_id\", concat(lit(\"h\"), col(\"show_id\")))\n",
    "netflix = titles4.withColumn(\"show_id\", concat(lit(\"n\"), col(\"show_id\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e56ae-ab50-4f45-8790-c4fe94d41076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unir dataframes\n",
    "\n",
    "titles = amazon.union(disneyPlus).union(hulu).union(netflix)\n",
    "titles = titles.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc4f83-5a77-4f2c-ae84-200b487fc9ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(titles.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719211fe-0d2c-4cab-a137-d61247092bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombrar \"type\" por \"show_type\"\n",
    "titles = titles.withColumnRenamed(\"type\", \"show_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee93fed-264b-40ae-90c4-a61352f2fcfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "titles.sample(False, 3/titles.count(), seed=150).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4a1a8-6d40-48a8-a8cb-b594ba74de79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# listar dimensiones dataframe titles\n",
    "\n",
    "numeroColumnasTitles = len(titles.columns)\n",
    "numeroFilasTitles = titles.count()\n",
    "print(\"Las dimensiones del DataFrame titles es de: ({}, {})\".format(numeroFilasTitles, numeroColumnasTitles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b5788f-8132-4cae-998d-9483a97012c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# listar valores nulos por columna\n",
    "\n",
    "titles.select(*(sum(col(c).isNull().cast('int')).alias(c) for c in titles.columns)).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72353b4e-73e1-4481-920f-9f835ec82b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eliminar fila nula, excepto show_id y columnas generadas\n",
    "\n",
    "titles = titles.na.drop(subset=['show_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb4ad7-da6e-4c8a-af84-ca70c665203e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#listar valores nulos por columna\n",
    "\n",
    "titles.select(*(sum(col(c).isNull().cast('int')).alias(c) for c in titles.columns)).pandas_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d549bbaf-68ad-46af-853b-ff1123174645",
   "metadata": {},
   "source": [
    "Los valores nulos del campo rating deberán reemplazarse por el string “G” (corresponde al maturity rating: “general for all audiences”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c8433-030c-45fa-88fb-610e33151170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titles = titles.fillna({\"rating\": \"G\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7088fa-38ce-490e-8e5c-cff676c587b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# listar valores nulos por columna\n",
    "\n",
    "titles.select(*(sum(col(c).isNull().cast('int')).alias(c) for c in titles.columns)).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a83541-9db0-4b85-b328-ba5b6501a4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reemplazar valores nulos de todo el titles por ''\n",
    "\n",
    "titles = titles.select([when(col(c).isNull(), '').otherwise(col(c)).alias(c) if c != 'id' else col(c) for c in titles.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d52c4d0-3bdb-47eb-80e6-0f5c5984a646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# listar valores nulos por columna\n",
    "\n",
    "titles.select(*(sum(col(c).isNull().cast('int')).alias(c) for c in titles.columns)).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b2e4c-eed8-4444-9ebc-3ece54e3bc0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "titles.sample(False, 3/titles.count(), seed=100).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1914f-d0db-42e6-b37a-83a9b9bfbc1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "titles.sample(False, 3/titles.count(), seed=150).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc71f0b-5cdd-49ae-8d34-3785d9b10fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# contar el número de valores nulos en cada columna\n",
    "titles.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in titles.columns]).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00776024-6023-4a00-a9c1-1cd28aa77b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# listar valores nulos por columna\n",
    "\n",
    "titles.select(*(sum(col(c).isNull().cast('int')).alias(c) for c in titles.columns)).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad3ddb-731a-4ba7-93e8-bebf61d03e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "titles.sample(False, 3/titles.count(), seed=150).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66809f22-13d8-43f9-9130-1e19a190470f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cambiar el formato del campo date_added a AAAA-mm-dd\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "\n",
    "titles = titles.withColumn(\"date_added\", date_format(to_date(\"date_added\", \"MMMM d, yyyy\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2309c2-b890-4d38-ab7d-6895f39f1986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "titles.sample(False, 3/titles.count(), seed=100).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796fd96-1f09-43f3-8785-f61d77cbb2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# listar tipo de dato\n",
    "\n",
    "for col in titles.columns:\n",
    "    print(col, \"es de tipo\", titles.schema[col].dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14643afc-b0e0-4efd-867c-415ac7340d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transformar a minusculas\n",
    "\n",
    "columnas = titles.columns\n",
    "\n",
    "for columna in columnas:\n",
    "    titles = titles.withColumn(columna, lower(columna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6363831b-7147-408e-b151-300c5db7e0b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transformar release a tipo de dato entero\n",
    "\n",
    "from pyspark.sql.functions import col as c\n",
    "\n",
    "titles = titles.withColumn('release_year', c('release_year').cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8edd4d-a5ad-4c32-9969-e66f8f3cb4be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Convertir la columna 'fecha' a formato fecha yyyy-MM-dd\n",
    "titles = titles.withColumn('date_added', to_date(col('date_added'), 'yyyy-MM-dd'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f17d99-e8bb-4bfb-a804-e9c201219e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "titles.sample(False, 3/titles.count(), seed=100).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8d3c4-d50a-414d-84b0-846908310f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convertir duration en dos campos: duration_int y duration_type\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "titles = titles.withColumn(\"duration_int\", split(\"duration\", \" \")[0])\n",
    "titles = titles.withColumn(\"duration_type\", split(\"duration\", \" \")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246870fc-1321-451d-ada3-010586f399d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "titles.sample(False, 3/titles.count(), seed=100).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e61432-20ce-4f2a-a705-3d28ce0b0b89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in titles.columns:\n",
    "    print(col, 'El tipo de dato es:', titles.schema[col].dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84ad91-52a6-48f7-ae58-c6072e1819da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(titles.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87469dc-7155-4d5d-99ab-86aa6b43ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tableA = titles.select(\"show_id\",\"show_type\", \"title\",\"cast\",\"country\",\"release_year\", \"rating\",\"platform\",\"duration_int\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357998e-d110-4ee0-b897-926c8a58d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tableA = tableA.filter((tableA[\"show_id\"] == \"\") | tableA[\"show_id\"].cast(\"string\").isNotNull())\n",
    "tableA = tableA.filter((tableA[\"show_type\"] == \"\") | tableA[\"show_type\"].cast(\"string\").isNotNull())\n",
    "tableA = tableA.filter((tableA[\"title\"] == \"\") | tableA[\"title\"].cast(\"string\").isNotNull())\n",
    "tableA = tableA.filter((tableA[\"cast\"] == \"\") | tableA[\"cast\"].cast(\"string\").isNotNull())\n",
    "tableA = tableA.filter((tableA[\"country\"] == \"\") | tableA[\"country\"].cast(\"string\").isNotNull())\n",
    "tableA = tableA.filter((tableA[\"release_year\"] == \"\") | tableA[\"release_year\"].cast(\"int\").isNotNull())\n",
    "tableA = tableA.filter((tableA[\"rating\"] == \"\") | tableA[\"rating\"].cast(\"string\").isNotNull())\n",
    "tableA = tableA.filter((tableA[\"platform\"] == \"\") | tableA[\"platform\"].cast(\"string\").isNotNull())\n",
    "tableA = tableA.filter((tableA[\"duration_int\"] == \"\") | tableA[\"duration_int\"].cast(\"int\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6084c8a-466d-4c9e-93ef-04131681bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "tableA.sample(False, 100/tableA.count(), seed=10).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc913b-f73f-45e6-af96-58737a0d4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c64576c-5396-4474-8d12-72c0cca5c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tableAPandas = tableA.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "31f19b7a-bb53-4ece-b3c2-457261f28f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tableAPandas.to_csv('tableA.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "910e5541-c1f8-4240-a03a-67eb4a3bf5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ae5b7e7-eb74-4a56-8dbe-b97381a80b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de datos SQLite 'Database.db' creada y datos insertados.\n"
     ]
    }
   ],
   "source": [
    "# Nombre del archivo de la base de datos SQLite\n",
    "sqlite_db = \"Database.db\"\n",
    "\n",
    "# Escribe el DataFrame de Pandas en la base de datos SQLite\n",
    "conn = sqlite3.connect(sqlite_db)\n",
    "tableAPandas.to_sql(\"tableA\", conn, if_exists=\"replace\", index=False)\n",
    "#df.to_sql(\"mi_tabla\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"Base de datos SQLite '{sqlite_db}' creada y datos insertados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9b66cded-7f2b-4401-a90e-6d4b13426b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import errorcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d2bb8d59-9b85-45be-bafa-b0590ccfb43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "\n",
    "# Configura la conexión al servidor MySQL (sin especificar una base de datos)\n",
    "try:\n",
    "    conn = mysql.connector.connect(\n",
    "        host='localhost',     # Reemplaza con la dirección del servidor de MySQL\n",
    "        user='root',          # Reemplaza con tu nombre de usuario de MySQL\n",
    "        password='patacon'    # Reemplaza con tu contraseña de MySQL\n",
    "    )\n",
    "\n",
    "    # Crea la base de datos si no existe\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"CREATE DATABASE IF NOT EXISTS streaming_mysql\")\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "    # Cierra la conexión temporal al servidor MySQL\n",
    "    conn.close()\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n",
    "        print(\"Error: Acceso denegado. Verifica tus credenciales de MySQL.\")\n",
    "    else:\n",
    "        print(f\"Error: {err}\")\n",
    "\n",
    "# Luego de crear la base de datos, puedes continuar con el código anterior para escribir el DataFrame\n",
    "# en la tabla de la base de datos MySQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6220e1f3-3443-4adc-af36-465d2bc7343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d8318168-4a2e-4062-ad8a-6c1ad7abb6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadenaConexion = \"mysql+pymysql://root:patacon@localhost:3306/streaming_mysql\"\n",
    "conexion = create_engine(cadenaConexion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dba2e95a-4428-46cf-8b7a-91bbb2c240cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22922"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tableAPandas.to_sql(name = \"tableA\", con = conexion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01cdc434-4fbb-44de-8d6c-c4e8d05122db",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'addJar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m conector \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmysql-connector-java-8.0.17-sources\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Cargar el conector JDBC de MySQL\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddJar\u001b[49m(conector)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'addJar'"
     ]
    }
   ],
   "source": [
    "conector = \"mysql-connector-java-8.0.17-sources\"\n",
    "# Cargar el conector JDBC de MySQL\n",
    "spark.sparkContext.addJar(conector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "067682e0-221e-4c8d-8dfb-ffcbb24affdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2660.save.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:246)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:250)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Configuración de las opciones de escritura\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[43mtableA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc:mysql://\u001b[39;49m\u001b[38;5;132;43;01m{localhost}\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;132;43;01m{3306}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{streaming_mysql}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtableA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatacon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# mode: \"append\", \"overwrite\", \"ignore\", \"error\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workSpace-Bwo4fINe/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1396\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1396\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1398\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workSpace-Bwo4fINe/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workSpace-Bwo4fINe/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workSpace-Bwo4fINe/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2660.save.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:246)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:250)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "# Configuración de las opciones de escritura\n",
    "tableA.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://{localhost}:{3306}/{streaming_mysql}\") \\\n",
    "    .option(\"dbtable\", \"tableA\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"patacon\") \\\n",
    "    .save()\n",
    "# mode: \"append\", \"overwrite\", \"ignore\", \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e2d2504-d930-448d-8a26-f4b42e6faf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named \"tableA\"\n",
    "#tableA.write.format(\"csv\").mode('overwrite').option(\"header\", \"true\").save(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a0567ef-e54a-45c6-afc9-ee8822995a78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_01 = titles.select(\"title\",\"show_type\", \"release_year\", \"duration_int\")\n",
    "query_01.write.format(\"csv\").mode('overwrite').option(\"header\", \"true\").save(\"query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d71d1708-d1a7-491a-8f04-651fb1fcf83d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#query.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "593f3f91-dde3-46ad-9213-1d0e3f061b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>show_type</th>\n",
       "      <th>release_year</th>\n",
       "      <th>duration_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>star trek: enterprise</td>\n",
       "      <td>tv show</td>\n",
       "      <td>2005</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c.h.u.d.</td>\n",
       "      <td>movie</td>\n",
       "      <td>1984</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>light in the dark</td>\n",
       "      <td>movie</td>\n",
       "      <td>2019</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title show_type  release_year duration_int\n",
       "0  star trek: enterprise   tv show          2005            4\n",
       "1               c.h.u.d.     movie          1984           97\n",
       "2      light in the dark     movie          2019          105"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "query_01.sample(False, 3/query_01.count(), seed=10).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56a59b65-679b-4b99-b673-11f82970cee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23018"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_01.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e09b62c3-5eb5-480b-9fd4-1ff51b2bc15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_01.write.format(\"csv\").mode('overwrite').option(\"header\", \"true\").save(\"query_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac9ed483-1759-4451-a05e-e4f3067df559",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_02 = ratings_scored.join(titles, ratings_scored.movieid == titles.show_id, how=\"inner\").select(ratings_scored.scored, titles.show_type, titles.release_year, titles.platform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6ea02689-0ed5-4f77-99c3-002e0bdabb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scored</th>\n",
       "      <th>show_type</th>\n",
       "      <th>release_year</th>\n",
       "      <th>platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.592050</td>\n",
       "      <td>movie</td>\n",
       "      <td>2014</td>\n",
       "      <td>netflix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.499000</td>\n",
       "      <td>movie</td>\n",
       "      <td>2011</td>\n",
       "      <td>netflix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.420949</td>\n",
       "      <td>movie</td>\n",
       "      <td>1995</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     scored show_type  release_year platform\n",
       "0  3.592050     movie          2014  netflix\n",
       "1  3.499000     movie          2011  netflix\n",
       "2  3.420949     movie          1995   amazon"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "query_02.sample(False, 3/query_02.count(), seed=10).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ca802c5-3ca2-4618-a7b7-5eb39781ff75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    }
   ],
   "source": [
    "query_02.write.format(\"csv\").mode('overwrite').option(\"header\", \"true\").save(\"query_02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5fe98668-51c4-479c-a196-897c22c2f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_03 = titles.select(\"show_id\", \"show_type\", \"platform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bc2b678c-4dac-4a60-b77e-de7ddb455a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_id</th>\n",
       "      <th>show_type</th>\n",
       "      <th>platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as908</td>\n",
       "      <td>tv show</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as2936</td>\n",
       "      <td>movie</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ns2514</td>\n",
       "      <td>movie</td>\n",
       "      <td>netflix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  show_id show_type platform\n",
       "0   as908   tv show   amazon\n",
       "1  as2936     movie   amazon\n",
       "2  ns2514     movie  netflix"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "query_03.sample(False, 3/query_03.count(), seed=10).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a1af6b8e-41ea-47be-8a90-78869662418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_03.write.format(\"csv\").mode('overwrite').option(\"header\", \"true\").save(\"query_03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e38f0bc7-be7a-4ab0-aaef-6dfaf329f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_04 = titles.select(\"show_type\", \"cast\",\"platform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c96b10aa-cf31-4a2c-b6e4-c523a02d5215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_type</th>\n",
       "      <th>cast</th>\n",
       "      <th>platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv show</td>\n",
       "      <td>scott bakula, jolene blalock, connor trinneer,...</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movie</td>\n",
       "      <td>daniel stern, kim greist</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie</td>\n",
       "      <td>rita dominic, joke silva, ngozi nwosu, kiki om...</td>\n",
       "      <td>netflix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  show_type                                                                                                              cast platform\n",
       "0   tv show  scott bakula, jolene blalock, connor trinneer, dominic keating, anthony montgomery, linda park, john billingsley   amazon\n",
       "1     movie                                                                                          daniel stern, kim greist   amazon\n",
       "2     movie                  rita dominic, joke silva, ngozi nwosu, kiki omeili, kalu ikeagwu, emmanuel essien, saidi balogun  netflix"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "query_04.sample(False, 3/query_04.count(), seed=10).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "818bea42-43d5-4318-a316-3cf71331aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_04.write.format(\"csv\").mode('overwrite').option(\"header\", \"true\").save(\"query_04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0151d83b-e1f8-40cf-8702-bf13c4b170eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_05 = titles.select(\"show_id\", \"show_type\", \"country\", \"release_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "821d5e17-c211-4b0d-8966-be33634e9efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_id</th>\n",
       "      <th>show_type</th>\n",
       "      <th>country</th>\n",
       "      <th>release_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as908</td>\n",
       "      <td>tv show</td>\n",
       "      <td></td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as2936</td>\n",
       "      <td>movie</td>\n",
       "      <td></td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ns2514</td>\n",
       "      <td>movie</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  show_id show_type  country  release_year\n",
       "0   as908   tv show                   2005\n",
       "1  as2936     movie                   1984\n",
       "2  ns2514     movie  nigeria          2019"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "query_05.sample(False, 3/query_05.count(), seed=10).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "63655a4e-7e5e-46a7-8ddf-298a09991279",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_05.write.format(\"csv\").mode('overwrite').option(\"header\", \"true\").save(\"query_05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "85488140-7dee-4909-8ff0-631d61057aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_06 = titles.select(\"title\",\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e1047c40-7d43-4464-88f4-5d10f5055080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>star trek: enterprise</td>\n",
       "      <td>tv-pg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c.h.u.d.</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>light in the dark</td>\n",
       "      <td>tv-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title rating\n",
       "0  star trek: enterprise  tv-pg\n",
       "1               c.h.u.d.      r\n",
       "2      light in the dark  tv-14"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# muestra aleatoria\n",
    "\n",
    "query_06.sample(False, 3/query_06.count(), seed=10).limit(3).pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d61a73fe-36f9-422e-813f-9eb495822dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_06.write.format(\"csv\").mode('overwrite').option(\"header\", \"true\").save(\"query_06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dca38138-6998-4890-93de-78cc99b0ee35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22998"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_02.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a9d88a-07ad-450a-bea8-bd3f99fe4f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dataframe__ = titlesFinal.join(dataFrameSistemaDeRecomendacion, titlesFinal.id == dataFrameSistemaDeRecomendacion.movieid, how='inner')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dceb04-1ebd-46aa-87b3-7fb2ed06fdc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12264c5b-8bb7-411b-b2fe-59ac0893f89d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f27587-4304-49e2-91b0-85b101cbe568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import avg\n",
    "\n",
    "#dataFrame = dataFrameSistemaDeRecomendacionReducido.groupBy('movieid').agg(avg('scored').alias('promedioScored'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78854be-6576-43bc-a979-07425b52cc66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a6f6a-7d8e-46dc-8256-d68a3e90ce82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbef6d9d-1042-4424-b50e-dd8097a8b9fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cantidad de usuarios\n",
    "\n",
    "#dataFrameSistemaDeRecomendacion.select(col('userid')).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f8d12f-03e7-4a87-b72b-9ea562156f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cantidad de peliculas y series\n",
    "\n",
    "#dataFrameSistemaDeRecomendacion.select(col('movieid')).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65cfb0f-08ce-4bd9-a676-51d21ec09296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dataFrameSistemaDeRecomendacion.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de479593-487f-4d7e-a742-d1b1537cf68e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# agrupar por movieid y obtener frecuecia\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "#cantidadVistasMovieId = dataFrameSistemaDeRecomendacion.groupBy(\"movieid\").agg(count(\"*\").alias(\"cantidad\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ac5df-b8c2-4194-a442-c3f4e5360801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calcular rango intercuatilico\n",
    "\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "primerCuartil, tercerCuartil = cantidadVistasMovieId.approxQuantile('cantidad', [0.25, 0.75], 0.01)\n",
    "rangoIntercuartilico = tercerCuartil - primerCuartil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2fe3e6-0579-4e2c-810d-a80ebf0f7b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Primer Caurtil', primerCuartil , 'Tercer Cuartil',tercerCuartil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca0b39b-2304-458b-898b-258ab030232a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "primerCuartil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec4b10-2436-4f22-ad22-098288ef617f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "limiteInferior = primerCuartil - 1.5 * rangoIntercuartilico\n",
    "limiteSuperior = tercerCuartil + 1.5 * rangoIntercuartilico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a92178-dab0-48a6-bced-8fe17ca514e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calcular cantidad de valores atipicos\n",
    "\n",
    "valoresAtipicos = cantidadVistasMovieId.filter(expr(\"cantidad > {0} or cantidad < {1}\".format(limiteSuperior, limiteInferior)))\n",
    "valoresAtipicos.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd832c-656c-4651-a0f2-44d39f1c914e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# movieid a eliminar con frecuencias inferiores a 419\n",
    "\n",
    "limiteInferior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ad33e-5efe-4189-84dc-76d18aca2ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cantidad de movieid inferiores al limite inferior\n",
    "\n",
    "valoresAtipicosDescartar = cantidadVistasMovieId.filter(expr(\"cantidad < {0}\".format(limiteInferior)))\n",
    "print('Cantidad de valores atipicos mas bajos al limite inferior:',valoresAtipicosDescartar.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94a64e-a766-46e9-941e-ebdacd58d03a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# listar la frecuencia de movieid y eliminar movieid con una frecuencia menor a 419\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "frecuencia = dataFrameSistemaDeRecomendacion.groupBy(\"movieid\").agg(count(\"*\").alias(\"frequency\"))\n",
    "frecuencia = frecuencia.filter(\"frequency >= 419\")\n",
    "dataFrameSistemaDeRecomendacionReducido = dataFrameSistemaDeRecomendacion.join(frecuencia, \"movieid\", \"inner\").drop(\"frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412a57f-20ca-42df-b72c-ff9f5bbde39d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lineas dataFrameSistemaDeRecomendacion\n",
    "\n",
    "dataFrameSistemaDeRecomendacion.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d9858-0a3b-463f-8424-27513dd01118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lineas dataFrameSistemaDeRecomendacionReducido\n",
    "\n",
    "dataFrameSistemaDeRecomendacionReducido.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ae42d-960f-42c1-a3de-61d15d156c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agrupar por movieid y obtener el promedio de scored\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "dataFrameSistemaDeRecomendacionFinal = dataFrameSistemaDeRecomendacionReducido.groupBy('movieid').agg(avg('scored').alias('promedioScored'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53724122-2e0e-4134-82a6-13c9953fab2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataFrameSistemaDeRecomendacionFinal.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea808b7-f3cc-47e2-ad54-5bf5842010c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# extraer la columna 'rating' como una lista\n",
    "ratings = dataFrameSistemaDeRecomendacion.select('rating').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# mostrar el histograma de la columna\n",
    "plt.hist(ratings, bins=5)\n",
    "\n",
    "plt.title('Distribución de calificaciones')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7d0391-5ccd-4897-bac9-bf4d08eda925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataFrameSistemaDeRecomendacionFinal.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a470c319-c0a2-457a-a3ed-12035b7057cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataFrameSistemaDeRecomendacionFinalPandas =dataFramePromedioScored.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5488eeaf-c4e2-482d-a80d-cd0717f1ce0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reducido.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f71285-92a0-46e5-8e5d-4f19f0908e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BinaryType\n",
    "\n",
    "# Función definida por el usuario para serializar el DataFrame\n",
    "def pickle_serialize(dataFrameSistemaDeRecomendacionFinal):\n",
    "    return pickle.dumps(dataFrameSistemaDeRecomendacionFinal)\n",
    "\n",
    "# Registro de la función udf en PySpark\n",
    "pickle_udf = udf(pickle_serialize, BinaryType())\n",
    "\n",
    "# Agregamos una nueva columna al DataFrame con el objeto serializado\n",
    "dataFrameSistemaDeRecomendacionFinal = dataFrameSistemaDeRecomendacionFinal.withColumn(\"serialized\", pickle_udf(dataFrameSistemaDeRecomendacionFinal))\n",
    "\n",
    "# Escribimos el DataFrame a un archivo de texto\n",
    "dataFrameSistemaDeRecomendacionFinal.select(\"serialized\").write.text(\"dataFrameSistemaDeRecomendacionFinal.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
